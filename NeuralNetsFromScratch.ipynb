{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOF1bl4UY3UxL9OeIxcZTf0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhythmd18/Neural-Nets-From-Scratch/blob/main/NeuralNetsFromScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's build a Neural Network Architecture from absolute scratch!"
      ],
      "metadata": {
        "id": "H345vbrK_Akg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Importing the desired libraries\n",
        "All we need here is the numpy library"
      ],
      "metadata": {
        "id": "XdbEijng_PK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "AywsCZtB-kIb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Formulating the required activation functions.\n",
        "We'll use the ReLU activation function for the hidden layers and the sigmoid activation function for the output layer.\n",
        "\n",
        "> The ReLU activation function is given by:\n",
        "$$g(z) = max(0, z)\\tag{1}$$\n",
        "\n",
        "> The sigmoid activation function is given by:\n",
        "$$g(z) = \\frac{1}{1 + e^{-z}}\\tag{2}$$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8vbjF6oq_bQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The activation functions\n",
        "\n",
        "def relu(Z):\n",
        "  \"\"\"\n",
        "  Implements the relu activation in numpy\n",
        "\n",
        "  Arguments:\n",
        "  Z: a numpy array\n",
        "\n",
        "  Returns:\n",
        "  A: the output of the relu activation function\n",
        "  cache: returns Z as well. useful during backpropagation\n",
        "  \"\"\"\n",
        "  A = np.maximum(0, Z)\n",
        "  cache = Z\n",
        "  return A, cache\n",
        "\n",
        "\n",
        "def sigmoid(Z):\n",
        "  \"\"\"\n",
        "  Implements the sigmoid activation in numpy\n",
        "\n",
        "  Arguments:\n",
        "  Z: a numpy array\n",
        "\n",
        "  Returns:\n",
        "  A: the output of the sigmoid activation function\n",
        "  cache: returns Z as well. useful during backpropagation\n",
        "  \"\"\"\n",
        "  A = 1 / (1 + np.exp(-Z))\n",
        "  cache = Z\n",
        "  return A, cache\n",
        "\n",
        "\n",
        "def relu_backward(dA, cache):\n",
        "  \"\"\"\n",
        "  Implements the backward propagation for a single ReLU unit\n",
        "\n",
        "  Arguments:\n",
        "  dA: post activation gradient\n",
        "  cache: Z where we store for computing backward propagation efficiently\n",
        "\n",
        "  Returns:\n",
        "  dZ: Gradient of the cost with respect to Z\n",
        "  \"\"\"\n",
        "  Z = cache\n",
        "  dZ = np.array(dA, copy=True)\n",
        "  dZ[Z <= 0] = 0 # When Z <= 0, dZ should be set to 0 as well\n",
        "  return dZ\n",
        "\n",
        "\n",
        "def sigmoid_backward(dA, cache):\n",
        "  \"\"\"\n",
        "  Implements the backward propagation for a single SIGMOID unit\n",
        "\n",
        "  Arguments:\n",
        "  dA: post activation gradient\n",
        "  cache: Z where we store for computing backward propagation efficiently\n",
        "\n",
        "  Returns:\n",
        "  dZ: Gradient of the cost with respect to Z\n",
        "  \"\"\"\n",
        "  Z = cache\n",
        "\n",
        "  s = 1 / (1 + np.exp(-Z))\n",
        "  dZ = dA * s * (1 - s)\n",
        "  return dZ"
      ],
      "metadata": {
        "id": "DpuUVIKB_ZhZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Initializing the parameter matrices of adequate shapes"
      ],
      "metadata": {
        "id": "bxy9B9XoEmaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(layer_dims):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  layer_dims: A Python list containing the number of units in each hidden layer\n",
        "\n",
        "  Returns:\n",
        "  parameters: A Python dictionary containing all the parameter matrices i.e., W1, b1, ..., WL, bL:\n",
        "              Wl: Weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "              bl: Bias matrix of shape (layer_dims[l], 1)\n",
        "  \"\"\"\n",
        "  np.random.seed(3)\n",
        "  parameters = {}\n",
        "  L = len(layer_dims) # Specifying the number of layers in the neural network\n",
        "\n",
        "  for l in range(1, L):\n",
        "    parameters[f\"W{str(l)}\"] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
        "    parameters[f\"b{str(l)}\"] = np.zeros((layer_dims[l], 1))\n",
        "\n",
        "  return parameters"
      ],
      "metadata": {
        "id": "GUA0OTwoF_4q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Forward Propagation"
      ],
      "metadata": {
        "id": "mZ_v5_bGLEJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Linear Forward\n",
        "Here we'll implement the linear part of the forward propagation i.e.,\n",
        "$$Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}\\tag{3}$$\n",
        "Here, $A^{[0]}=X$, where $X$ is the input vector."
      ],
      "metadata": {
        "id": "M0Z9L_J6LLYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_forward(A, W, b):\n",
        "  \"\"\"\n",
        "  Implements the linear part of a layer's forward propagation\n",
        "\n",
        "  Arguments:\n",
        "  A: activations from the previous layer of shape (size of previous layer, number of examples)\n",
        "  W: weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "  b: bias vector: numpy array of shape (size of current layer, 1)\n",
        "\n",
        "  Returns:\n",
        "  Z: the linear part or the input for the activation function\n",
        "  cache: a python tuple containing \"A\", \"W\", and \"b\"; stored for computing backward pass efficiently\n",
        "  \"\"\"\n",
        "  Z = np.dot(W, A) + b\n",
        "  cache = (A, W, b)\n",
        "\n",
        "  return Z, cache"
      ],
      "metadata": {
        "id": "x0FR1tbcE83a"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Linear Activation Forward\n",
        "Now we'll pass the linear part Z as input to the activation function and compute the activations (sigmoid or relu)"
      ],
      "metadata": {
        "id": "JjSWU8o6O1K3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "  \"\"\"\n",
        "  Implements the forward propagation for the LINEAR->ACTIVATION layer\n",
        "\n",
        "  Arguments:\n",
        "  A_prev: activations from the previous layer of shape (size of the previous layer, number of examples)\n",
        "  W: weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "  b: bias vector: numpy array of shape (size of current layer, 1)\n",
        "  activation: the activation to be used in the current layer, stored as a string (\"sigmoid\" or \"relu\")\n",
        "\n",
        "  Returns:\n",
        "  A: the output of the activation function, which is then used to compute the linear part Z of the next layer\n",
        "  cache: a python tuple containing \"linear_cache\" and \"activation_cache\"; stored for computing backward pass efficiently\n",
        "  \"\"\"\n",
        "  if activation == \"sigmoid\":\n",
        "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "    A, activation_cache = sigmoid(Z)\n",
        "\n",
        "  elif activation == \"relu\":\n",
        "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "    A, activation_cache = relu(Z)\n",
        "\n",
        "  cache = (linear_cache, activation_cache)\n",
        "\n",
        "  return A, cache"
      ],
      "metadata": {
        "id": "ttRceG3QE9S3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 L-Layer Model\n",
        "Here we'll build the neural network architecture with L number of layers. L-1 hidden layers will use the <b>ReLU</b> activation and the output layer will use the <b>sigmoid</b> activation."
      ],
      "metadata": {
        "id": "K2v3Yjz0SZZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def l_layer_model(X, parameters):\n",
        "  \"\"\"\n",
        "  Implements the forward propagation for [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
        "\n",
        "  Arguments:\n",
        "  X: input vector of shape (input size, number of examples)\n",
        "  parameters: output of the function initialize_parameters()\n",
        "\n",
        "  Returns:\n",
        "  AL: activation value of the output layer\n",
        "  caches: list of caches containing:\n",
        "            every cache of the linear_activation_forward()\n",
        "  \"\"\"\n",
        "  caches = []\n",
        "  A = X\n",
        "  L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "  for l in range(1, L):\n",
        "    A_prev = A\n",
        "    A, cache = linear_activation_forward(A_prev, parameters[f'W{str(l)}'], parameters[f'b{str(l)}'], 'relu')\n",
        "    caches.append(cache)\n",
        "\n",
        "  AL, cache = linear_activation_forward(A, parameters[f'W{str(L)}'], parameters[f'b{str(L)}'], 'sigmoid')\n",
        "  caches.append(cache)\n",
        "\n",
        "  return AL, caches"
      ],
      "metadata": {
        "id": "CTuKxuR6E9iX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Cost Function\n",
        "We use the following formula for computing the cost\n",
        "$$J = -\\frac{1}{m}\\sum (y^{(i)}log(a^{[L](i)})+(1-y^{(i)})log(1-a^{[L](i)}))\\tag{4}$$"
      ],
      "metadata": {
        "id": "NqUgfQGtDbKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(AL, Y):\n",
        "  \"\"\"\n",
        "  Implements the cost defined by the equation (4)\n",
        "\n",
        "  Arguments:\n",
        "  AL: the probability vector corresponding to the label predictions of shape (1, number of examples)\n",
        "  Y: actual labels vector\n",
        "\n",
        "  Returns:\n",
        "  cost: the cross-entropy cost\n",
        "  \"\"\"\n",
        "  m = Y.shape[1]\n",
        "\n",
        "  cost = -(np.sum(np.dot(Y, np.log(AL).T) + np.dot(1 - Y, np.log(1 - AL).T))) / m\n",
        "  cost = np.squeeze(cost)\n",
        "\n",
        "  return cost"
      ],
      "metadata": {
        "id": "i4yvmhN9E9rn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Backward Propagation"
      ],
      "metadata": {
        "id": "V2QR6hh5KKs2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Linear Backward\n",
        "For layer $l$, the linear part is $Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$\n",
        "Suppose, we have calculated the derivative of $Z$ i.e $dZ^{[l]} = \\frac {\\partial L}{\\partial Z^{[l]}}$, we'll need to get ($dW^{[l]}, db^{[l]}, dA^{[l-1]}$)\n",
        "\n",
        "The formulas are as follows:\n",
        "\n",
        "$$dW^{[l]}=\\frac{1}{m}dZ^{[l]}A^{[l-1]T}\\tag{5}$$\n",
        "\n",
        "$$db^{[l]}=\\frac{1}{m}\\sum_{i=1}^{m}dZ^{[l](i)}\\tag{6}$$\n",
        "\n",
        "$$dA^{[l-1]}=W^{[l]T}dZ^{[l]}\\tag{7}$$"
      ],
      "metadata": {
        "id": "GXF7zPIlKzdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_backward(dZ, cache):\n",
        "  \"\"\"\n",
        "  Implements the linear portion of backward propagation\n",
        "\n",
        "  Arguments:\n",
        "  dZ: gradient of the cost with respect to the linear output(of the current layer)\n",
        "  cache: tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "  Returns:\n",
        "  dA_prev: gradient of the cost with respect to the activation of the previous layer (l-1)\n",
        "  dW: gradient of the cost with respect to W of the current layer l\n",
        "  db: gradient of the cost with respect to b of the current layer l\n",
        "  \"\"\"\n",
        "  A_prev, W, b = cache\n",
        "  m = A_prev.shape[1]\n",
        "\n",
        "  dW = np.dot(dZ, A_prev.T) / m\n",
        "  db = np.sum(dZ, axis=1, keepdims=True) / m\n",
        "  dA_prev = np.dot(W.T, dZ)\n",
        "\n",
        "  return dA_prev, dW, db"
      ],
      "metadata": {
        "id": "iFZS_G9BE9zW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Linear Activation Backward\n",
        "If g(.) is the activation then `sigmoid_backward` and `relu_backward` compute the following equation:\n",
        "$$dZ^{[l]}=dA^{[l]}*g'(Z^{[l]})\\tag{8}$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YMLMwrvGmg-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_activation_backward(dA, cache, activation):\n",
        "  \"\"\"\n",
        "  Implements the backward propagation for a single LINEAR-> ACTIVATION layer\n",
        "\n",
        "  Arguments:\n",
        "  dA: post activation gradient of the current layer l\n",
        "  cache: tuple of values(linear_cache, activation_cache) we store to compute backward pass efficiently\n",
        "  activation: string that stores the name of the activation function that specifies which one to use.\n",
        "\n",
        "  Returns:\n",
        "  dA_prev: gradient of cost with respect to of the previous layer (l - 1)\n",
        "  dW: gradient of the cost with respect to W(current layer l)\n",
        "  db: gradient of the cost with respect to b(current layer l)\n",
        "  \"\"\"\n",
        "  linear_cache, activation_cache = cache\n",
        "\n",
        "  if activation == 'relu':\n",
        "    dZ = relu_backward(dA, activation_cache)\n",
        "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "  if activation == 'sigmoid':\n",
        "    dZ = sigmoid_backward(dA, activation_cache)\n",
        "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "  return dA_prev, dW, db"
      ],
      "metadata": {
        "id": "LpdJ6YSxE96L"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 L-Model Backward\n",
        "Here we'll perform the entire backward propagation computing the gradients going backwards for each layer."
      ],
      "metadata": {
        "id": "bhDflqUin_uW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def L_model_backward(AL, Y, caches):\n",
        "  \"\"\"\n",
        "  Implements backward propagation for the entire LINEAR->RELU * (L - 1) + LINEAR->SIGMOID group\n",
        "\n",
        "  Arguments:\n",
        "  AL: probability vector, the final output of the forward propagation(L_model_forward())\n",
        "  Y: the true labels vector\n",
        "  cache: list of caches for all the layers. For each layer it contains the tuple(linear_cache, activation_cache)\n",
        "\n",
        "\n",
        "  Returns:\n",
        "  grads: dictionary with the gradients\n",
        "  \"\"\"\n",
        "  grads = {}\n",
        "  L = len(caches) # number of layers\n",
        "  m = AL.shape[1]\n",
        "  Y = Y.reshape(AL.shape) # Y should be of the same shape as AL\n",
        "\n",
        "  dAL = -np.divide(Y, AL) + np.divide(1 - Y, 1 - AL)\n",
        "\n",
        "  current_cache = caches[L - 1]\n",
        "  dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, 'sigmoid')\n",
        "  grads[f\"dA{str(L - 1)}\"] = dA_prev_temp\n",
        "  grads[f\"dW{str(L)}\"] = dW_temp\n",
        "  grads[f\"db{str(L)}\"] = db_temp\n",
        "\n",
        "  for l in reversed(range(L - 1)):\n",
        "    current_cache = caches[l]\n",
        "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[f\"dA{str(l + 1)}\"], current_cache, 'relu')\n",
        "    grads[f\"dA{str(l)}\"] = dA_prev_temp\n",
        "    grads[f\"dW{str(l + 1)}\"] = dW_temp\n",
        "    grads[f\"db{str(l + 1)}\"] = db_temp\n",
        "\n",
        "  return grads"
      ],
      "metadata": {
        "id": "Y3cTCT0tn-PO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Update Parameters\n",
        "In this section, we'll update all of the parameters as per the gradient descent algorithm\n",
        "$$W^{[l]}=W^{[l]}-\\alpha\\text{ }dW^{[l]}\\tag{9}$$\n",
        "$$b^{[l]}=b^{[l]}-\\alpha\\text{ }db^{[l]}\\tag{10}$$"
      ],
      "metadata": {
        "id": "MZ_Xk2_EsDKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(params, grads, learning_rate):\n",
        "  \"\"\"\n",
        "  Implements the process of gradient descent by updating the parameters\n",
        "\n",
        "  Arguments:\n",
        "  params: the initial parameters (Output of initialize_parameters()) dictionary\n",
        "  grads: the dictionary containing the gradients\n",
        "  learning_rate: alpha which determines the speed of the updation\n",
        "\n",
        "  returns:\n",
        "  parameters: the dictionary of the updated parameters\n",
        "  \"\"\"\n",
        "  parameters = params.copy()\n",
        "  L = len(parameters) // 2 # number of layers\n",
        "\n",
        "  for l in range(L):\n",
        "    parameters[f\"W{str(l + 1)}\"] = params[f\"W{str(l + 1)}\"] - learning_rate * grads[f\"dW{str(l + 1)}\"]\n",
        "    parameters[f\"b{str(l + 1)}\"] = params[f\"b{str(l + 1)}\"] - learning_rate * grads[f\"db{str(l + 1)}\"]\n",
        "\n",
        "  return parameters"
      ],
      "metadata": {
        "id": "YJ-1CWuCsGLS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LXTLAFtOszof"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}